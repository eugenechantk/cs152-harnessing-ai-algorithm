{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIFour: Computer AI Playing Connect Four\n",
    "\n",
    "### What is Connect Four\n",
    "Connect Four is a two-player game where the aim is to have four of the same colored disc horizontally, vertically or diagonnally. Players will take turn to place their own colored disc on one of the seven columns in the 7x6 board. The discs of the same column stack on top of each other.\n",
    "\n",
    "It is a zero-sum game because either player would win, or both would draw the game. It is also a complete game because each player has perfect information on what the current board look like and what are the available moves for the other player.\n",
    "\n",
    "### What makes Connect Four an interesting AI search problem?\n",
    "Although there is limited steps that each player can take (at most 7, unless the board has a different number of columns), the number of possible state of the board can be up to four trillion. \n",
    "\n",
    "To build a computer bot that can play against a human player, it has to understand the board and determine a move in a reasonably short time. Therefore it is impossible to search through all the possible states before determining the move\n",
    "\n",
    "### How can AI search algorithms be applied in this bot?\n",
    "This AI bot deploys either of the two search algorithms: NegaMax (a variance of the MiniMax algorithm) and Monte Carlo Tree Search. These two search algorithms does a partial search on all the possible steps given a state of the board and return the best move it can find. While one is breadth-first search (Monte Carlo Tree Search), while the other is depth-first search (NegaMax), both are able to search through a large number of board states and determine the best step given the steps simulated.\n",
    "\n",
    "Using the tree search method that is implemented in both Monte Carlo Tree Search and NegaMax can allow the computer to be methodological in approaching the possible steps, simulate many steps ahead of the current state, and retrace the steps that lead to the best outcome.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Four Class\n",
    "Establishing the main methods for createing, playing and verifying the game. The state of the board is representedas a 7x6 matrix, with \"1\" indicating disc from Player 1, and \"2\" indicating disc from Player 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConnectFour():\n",
    "    \n",
    "    ROW = 6\n",
    "    COLUMN = 7\n",
    "    INITIAL_STATE = [[],] * COLUMN\n",
    "    PLAYERS = (1,2)\n",
    "    GOAL = 4\n",
    "    VALUE_WIN = 1\n",
    "    VALUE_LOSE = -1\n",
    "    VALUE_DRAW = 0\n",
    "    \n",
    "    #For Negamax algorithm implementation\n",
    "    STREAK_WEIGHT = [1, 8, 128, 99999]\n",
    "    evaluated = {}\n",
    "    \n",
    "    def __init__(self,row=ROW,column=COLUMN,state=INITIAL_STATE,player=PLAYERS,goal=GOAL):\n",
    "        self.row = row\n",
    "        self.column = column\n",
    "        self.state = state\n",
    "        self.player = player\n",
    "        self.goal = goal\n",
    "    \n",
    "    def legal(self,state,action):\n",
    "        if len(self.state[action]) >= self.row:\n",
    "            raise Exception(\"Invalid action: columnn is full!\")\n",
    "        else:\n",
    "            return len(self.state[action]) < self.row\n",
    "        \n",
    "    def actions(self, state):\n",
    "        return [i for i in xrange(len(state)) if self.legal(state, i)]\n",
    "    \n",
    "    def update_state(self, state, action, player):\n",
    "        if not self.legal(state, action):\n",
    "            raise Exception('Illegal action')\n",
    "        newstate = []\n",
    "        for index, column in enumerate(state):\n",
    "            if index == action:\n",
    "                newstate.append(column + [player])\n",
    "            else:\n",
    "                newstate.append(column)\n",
    "        #newstate[len(state)-1] = player\n",
    "        return newstate\n",
    "    \n",
    "    def streak(self,state,player,start,delta,length=0):\n",
    "        row, column = start\n",
    "        if row < 0 or column < 0:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            piece = state[column][row]\n",
    "        except IndexError:\n",
    "            return False\n",
    "        \n",
    "        if piece != player:\n",
    "            return False\n",
    "        \n",
    "        # Current slot is owned by the player\n",
    "        length += 1\n",
    "        if length == self.goal: # Streak is already long enough\n",
    "            return True\n",
    "        # Continue searching, \n",
    "        drow, dcolumn = delta\n",
    "        \n",
    "        return self.streak(state,player,(row + drow, column + dcolumn),delta,length)        \n",
    "            \n",
    "    \n",
    "    def outcome(self, state, player):\n",
    "        for col, column in enumerate(state):\n",
    "            for row, marker in enumerate(column):\n",
    "                if any((\n",
    "                    self.streak(state, marker, (row, col), (1, 0)),  #check upwards\n",
    "                    self.streak(state, marker, (row, col), (0, 1)),  #check rightward \n",
    "                    self.streak(state, marker, (row, col), (1, 1)),  #check right diagonal\n",
    "                    self.streak(state, marker, (row, col), (1, -1)), #check left diagonal\n",
    "                )):\n",
    "                    # A winner was found\n",
    "                    if marker == player:\n",
    "                        return self.VALUE_WIN\n",
    "                    else:\n",
    "                        return self.VALUE_LOSE\n",
    "        # No winner was found\n",
    "        return self.VALUE_DRAW\n",
    "    \n",
    "    def terminal(self, state):\n",
    "        if all([len(column) == self.row for column in state]):  #when all columns are full\n",
    "            return True\n",
    "        if self.outcome(state, self.player[0]) != self.VALUE_DRAW:  #when there is a winner\n",
    "            return True\n",
    "        return False  #the board still have space and the game is still on\n",
    "    \n",
    "    def next_player(self, player):\n",
    "        if player not in self.player:\n",
    "            raise Exception('Invalid player')\n",
    "        index = self.player.index(player)\n",
    "        if index < 1:\n",
    "            return self.player[index + 1]\n",
    "        else:\n",
    "            return self.player[0]\n",
    "    \n",
    "    def pretty_state(self, state, escape=False):\n",
    "        output = ''\n",
    "        for j in range(self.column):\n",
    "            output += ' ' + str(j)\n",
    "        output += ' '\n",
    "        if escape:\n",
    "            output += '\\\\n'\n",
    "        else:\n",
    "            output += '\\n'\n",
    "        #boardState = state[1:self.column-1]\n",
    "        i = self.row - 1\n",
    "        while i >= 0:\n",
    "            for column in state:\n",
    "                if len(column) > i:\n",
    "                    output += '|' + str(column[i])\n",
    "                else:\n",
    "                    output += '| '\n",
    "            output += '|'\n",
    "            if escape:\n",
    "                output += '\\\\n'\n",
    "            else:\n",
    "                output += '\\n'\n",
    "            i -= 1\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    #For Negamax algorithm implementation\n",
    "    def streak_eval(self,state,player,start,delta,length=0,max_length = 0):\n",
    "        row, column = start\n",
    "        if row < 0 or column < 0:\n",
    "            return max_length\n",
    "        \n",
    "        try:\n",
    "            piece = state[column][row]\n",
    "        except IndexError:\n",
    "            return max_length\n",
    "        \n",
    "        if piece != player:\n",
    "            length = 0\n",
    "        \n",
    "        # Current slot is owned by the player\n",
    "        length += 1\n",
    "        if length > max_length: # if current streak is longer than the max_length\n",
    "            max_length = length\n",
    "        # Continue searching, \n",
    "        drow, dcolumn = delta\n",
    "        \n",
    "        if row == self.row and column == self.column:\n",
    "            return max_length\n",
    "        \n",
    "        return self.streak_eval(state,player,(row + drow, column + dcolumn),delta,length,max_length)\n",
    "    \n",
    "    def evaluate(self,state,player):\n",
    "        curPlayer = player\n",
    "        curPlayerValue = 0\n",
    "        otherPlayerValue = 0\n",
    "        \n",
    "        for col, column in enumerate(state):\n",
    "            for row, marker in enumerate(column):\n",
    "                max_length_up = self.streak_eval(state, marker, (row, col), (1, 0))  #check upwards\n",
    "                max_length_right = self.streak_eval(state, marker, (row, col), (0, 1))  #check rightward \n",
    "                max_length_diaright = self.streak_eval(state, marker, (row, col), (1, 1))  #check right diagonal\n",
    "                max_length_dialeft = self.streak_eval(state, marker, (row, col), (1, -1)) #check left diagonal\n",
    "                if marker == curPlayer:\n",
    "                    max_length = max(max_length_up,max_length_right,max_length_diaright,max_length_dialeft)\n",
    "                    curPlayerValue += self.STREAK_WEIGHT[max_length-1]\n",
    "                    #print \"Player two value updated: max_length:{} player_2_value:{}\\n\".format(max_length, player_2_value)\n",
    "                else:\n",
    "                    max_length = max(max_length_up,max_length_right,max_length_diaright,max_length_dialeft)\n",
    "                    otherPlayerValue += self.STREAK_WEIGHT[max_length-1]\n",
    "                    #print \"Player one value updated: max_length:{} player_1_value:{}\\n\".format(max_length, player_1_value)\n",
    "        \n",
    "        difference = curPlayerValue - otherPlayerValue \n",
    "        return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implemented from my past assignment\n",
    "#Use to flatten the state of the 2d board so to allow hashing\n",
    "def flatten(board):\n",
    "    # if it's nested lists, flatten them. I do this with list comprehension taking each tile at a time from each sublist\n",
    "    if type(board[1])==list:\n",
    "        board = [item for sublist in board for item in sublist] \n",
    "    # else, it should be a list of ints or floats\n",
    "    elif type(board[1])==int or type(board[1])==float: \n",
    "        board = board\n",
    "    # if it's neither, it's a wrong input and will raise an error.\n",
    "    else:\n",
    "        raise ValueError(\"Class 'PuzzleNode' got values that are not a sublist of ints nor a flat list of ints.\")\n",
    "    return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Methods\n",
    "#### __init__()\n",
    "Create a board given the board size (row and column), the current state (or a state with no disc), the current player (default is the first player) and the goal of the game (default is 4, as in connecting four of the same-colored disc)\n",
    "\n",
    "#### legal()\n",
    "Determine whether the action parsed can be performed given the current state (an action cannot be performed if the column is full). Return **TRUE** if the action can be performed\n",
    "\n",
    "#### actions()\n",
    "Given the state, return a list of available actions that the current player can perform\n",
    "\n",
    "#### update_state()\n",
    "Update the state of the board when given a move and the player. Add the player number to the representation of the board\n",
    "\n",
    "#### streak()\n",
    "Auxilary function to evaluate whether the board has a player that connected four. It is called on the *outcome()* function\n",
    "\n",
    "#### outcome()\n",
    "Returns the value of the outcome given the player and the state: 1 = the player won; -1 = the player lose; 0 = they draw\n",
    "\n",
    "#### terminal()\n",
    "Check if the board is full, or there is a player given the current state. Returns **TRUE** either there is a winner, or the game draws\n",
    "\n",
    "#### next_player()\n",
    "Given the current player, return the number of the next player so to determine legal moves and update the state once hte player has moved\n",
    "\n",
    "#### pretty_state()\n",
    "Given the board, show a visual version of the current state of the board\n",
    "\n",
    "#### streak_eval() and evaluate()\n",
    "Auxilary functions for NegaMax search algorithm implementation (these two functions will be discussed in the later section)\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Class\n",
    "Establishing the leaf nodes of the search tree, given the current state, the action played, the player playing, the game the player is playing, the parent of the leaf node.\n",
    "\n",
    "When using NegaMax, it will also specify the maximum depth that the algorithm will branch into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt, log\n",
    "import random\n",
    "\n",
    "class MonteCarloNode():\n",
    "    \n",
    "    def __init__(self,state,action,player,cur_game=None,parent=None,max_depth=None):\n",
    "        self.game = game or parent.game\n",
    "        \n",
    "        #About the parents and children of the node\n",
    "        self.parent = parent\n",
    "        self.children = dict.fromkeys(self.game.actions(state))\n",
    "        \n",
    "        #About the move of the node\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        \n",
    "        #About the player and the reward of the node\n",
    "        self.player = player\n",
    "        self.visit = 0.0\n",
    "        self.value = 0.0\n",
    "        self.weight = 0.0\n",
    "        \n",
    "        #For Negamax algorithm implementation\n",
    "        self.nega_value = 0\n",
    "        self.max_depth = max_depth\n",
    "        self.flatten_state = flatten(self.state)\n",
    "        self.stateId = hash(tuple(self.flatten_state))\n",
    "    \n",
    "    def __hash__(self):\n",
    "        if self.stateId is None:\n",
    "            self.stateId = hash(tuple(self.flatten_state))\n",
    "        return self.stateId\n",
    "    \n",
    "    def _tree_policy(self):\n",
    "        bestNode = self\n",
    "        while not bestNode.game.terminal(self.state):\n",
    "            if not bestNode._fully_expanded():\n",
    "                return bestNode._expand()\n",
    "            else:\n",
    "                bestNode = bestNode._best_child()\n",
    "        return bestNode\n",
    "    \n",
    "    def _expand(self):\n",
    "        try:\n",
    "            expAction = self.children.keys()[self.children.values().index(None)]\n",
    "        except ValueError:\n",
    "            raise Exception('Node is already fully expanded')\n",
    "        \n",
    "        expState = self.game.update_state(self.state,expAction,self.player)\n",
    "        expPlayer = self.game.next_player(self.player)\n",
    "        \n",
    "        child = MonteCarloNode(expState,expAction,expPlayer,self.game,self)\n",
    "        self.children[expAction] = child\n",
    "        return child\n",
    "    \n",
    "    def _fully_expanded(self):\n",
    "        return not None in self.children.values()\n",
    "    \n",
    "    def _weight(self):\n",
    "        if self.visit == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return self.value/self.visit\n",
    "    \n",
    "    def _default_policy(self,player):\n",
    "        game = self.game\n",
    "        curState = self.state\n",
    "        curPlayer = self.player\n",
    "        while not game.terminal(curState):\n",
    "            curAction = random.choice(game.actions(curState))\n",
    "            curState = game.update_state(curState,curAction,curPlayer)\n",
    "            curPlayer = game.next_player(curPlayer)\n",
    "        reward = game.outcome(curState,player)\n",
    "        return reward\n",
    "        \n",
    "    def _backup(self,reward,budget,pntlevel=0):\n",
    "        backupNode = self\n",
    "        search_depth = 1\n",
    "        while not backupNode is None:\n",
    "            backupNode.value += reward\n",
    "            backupNode.visit += 1\n",
    "            search_depth += 1\n",
    "            backupNode = backupNode.parent\n",
    "            \n",
    "            if pntlevel != 0:\n",
    "                if budget % 100 == 0:\n",
    "                    print backupNode\n",
    "        \n",
    "        return search_depth\n",
    "            \n",
    "    def _best_child(self,c=1/sqrt(2)):\n",
    "        return max(self.children.values(), key=lambda x: x._search_weight(sqrt(c)))\n",
    "    \n",
    "    def _search_weight(self, c):\n",
    "        return self._weight() + c * sqrt(2 * log(self.parent.visit) / self.visit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Functions\n",
    "### Functions for NegaMax \n",
    "#### __hash__()\n",
    "Hash the leaf node using the current state of the board so the leaf node and its content can be retrieve in the future (used in NegaMax search algorithm)\n",
    "\n",
    "### Functions for Monte Carlo Tree Search\n",
    "#### _tree_policy()\n",
    "**Tree policy** of the Monte Carlo Tree Search (explanation in the later section)\n",
    "\n",
    "#### _expand()\n",
    "Given the state and its available action, randomly select one and create a child node based on the new state after exercising that action\n",
    "\n",
    "#### _fully_expand()\n",
    "Return **TRUE** if the current node is fully populated with child nodes\n",
    "\n",
    "#### _default_policy()\n",
    "**Default policy** of the Monte Carlo Tree Search (explanation in the later section)\n",
    "\n",
    "#### _weights()\n",
    "Assign weights to the node based on this formula: reward/visit  \n",
    "Reward: +1 if the player wins, -1 if the player loses, 0 if its draw; accumulated based on outcome of the child node)  \n",
    "Visit: number of times this node has been visited (accounting for repeated visits if it is not a leaf node, but an intermediate node)\n",
    "\n",
    "#### _search_weight()\n",
    "A search function that assigns weights to nodes not only just by the reward/visit formula, but also account for the number of times the node has been visited. Used in the UCT variant of the Monte Carlo Tree Search (explanation in the later section)\n",
    "\n",
    "#### _backup()\n",
    "**Backward propagation** of the Monte Carlo Tree Search (Explanation in the later section)\n",
    "\n",
    "#### _best_child()\n",
    "Return the child node with the best search weights given by the *_search_weight()* function\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nega-Max for Connect Four\n",
    "\n",
    "### What is the Nega-max Search Algorithm\n",
    "NegaMax search algorithm is a variation of the MiniMax search algorithm. The MiniMax search algorithm seeks to maximize the utility function when its the algorithm's turn to play, the minimize the utility when its the other player's turn (hence maximizing the algorithm's utility).\n",
    "\n",
    "The algorithm simulates the possible steps by alternating players, until it reaches a terminating stage (win, lose or draw)\n",
    "\n",
    "NegaMax has the same aim of maximizing the algorithm's utility during its turn, and minimize the utility during the other player's turn. But it is a simplified version of MiniMax, utilizing the fact that in connect four, the outcome is dichotomic, meaning that the minimum utility is the negative (losing has the utility of -N) of the maximum utility (wining has the utility of N)\n",
    "\n",
    "### Inner Working of the Nega-max Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns the best node (and best score) for a given state\n",
    "def negamax(game,state,player,depth=0,max_depth=4,parent=None,action=None,s_steps=0,s_depth=0):\n",
    "    current = MonteCarloNode(state,action,player,game,parent,max_depth)\n",
    "    \n",
    "    search_step = s_steps\n",
    "    search_depth = s_depth\n",
    "    \n",
    "    search_step += 1\n",
    "    \n",
    "    if current in game.evaluated:\n",
    "        return (current, current.nega_value,search_step,search_depth)\n",
    "    \n",
    "    if depth == current.max_depth:\n",
    "        score = current.game.evaluate(current.state,current.player)\n",
    "        game.evaluated[current] = score\n",
    "        return (current, score,search_step,search_depth)\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    best_node = None\n",
    "    \n",
    "    for nextAction in game.actions(current.state):\n",
    "        nextState = game.update_state(current.state,nextAction,current.player)\n",
    "        \n",
    "        if game.terminal(nextState):\n",
    "            outcome = game.outcome(nextState,player)\n",
    "            if outcome == game.VALUE_WIN:\n",
    "                current.nega_value = 99999\n",
    "            elif outcome == game.VALUE_LOSE:\n",
    "                current.nega_value = -99999\n",
    "            else:\n",
    "                current.nega_value = 0\n",
    "            best_subscore = current.nega_value\n",
    "            nextPlayer = game.next_player(current.player)\n",
    "            #create MonteCarloNode to retrieve the action at this level\n",
    "            best_subnode = MonteCarloNode(nextState,nextAction,nextPlayer,game,current,max_depth)\n",
    "        \n",
    "        else:\n",
    "            nextPlayer = game.next_player(current.player)\n",
    "            newDepth = depth + 1\n",
    "            \n",
    "            (best_subnode, best_subscore, search_step, search_depth) = negamax(game,nextState,nextPlayer,newDepth,max_depth,current,nextAction,search_step,search_depth)\n",
    "            best_subscore *= -1\n",
    "        \n",
    "        if best_subscore > best_score:\n",
    "            best_score = best_subscore\n",
    "            best_node = best_subnode\n",
    "    \n",
    "    if best_node is None:\n",
    "        best_score = game.evaluate(current.state,player)\n",
    "    \n",
    "    search_depth += 1\n",
    "    \n",
    "    game.evaluated[current] = best_score\n",
    "    \n",
    "    return (best_node, best_score, search_step, search_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NegaMax works with a recursive call. At every call, it will create a leaf node based on a legal action. If the state of the leaf node is not a win, lose or draw, it will branch again and form a child node with another action.  \n",
    "\n",
    "Procedure for NegaMax Search Algorithm:\n",
    "1. Create a leaf node based on the state and player parsed when Negamax is called  \n",
    "2. If the node has been evaluated, return the evaluated node and its utility  \n",
    "3. If the current depth of search reaches the maximum depth designated, and the game has not terminate yet, the utility will be given as the value of a heuristic function (more about the heuristic function in the next subsection)  \n",
    "4. Update the state based on the action and state of the current child node, and evaluate whether the game has terminated yet  \n",
    "4.1 If the game has terminated, assign the utility of the current child (Win = 99999; Lose = -99999; Draw = 0)\n",
    "4.2 If the game has not terminated, repeat step 1-4, but with the next player\n",
    "5. Assign the best_utility\n",
    "6. Store the current board in the evaluated dictionary for future retrieval\n",
    "7. Return the node with the best utility, so that we can retrieve the action needed to achieve the best utility\n",
    "\n",
    "### What if the game does not terminate after the algorithm reaches maximum search depth?\n",
    "As mentioned above, not every path will lead to a terminal state when the maximum search depth is reached. In that case, we cannot assign a simplistic win-lose-draw utility to that particular leaf node. \n",
    "\n",
    "Here we introduce a heuristic that assign utility based on the number of chains in the current state for each player. The detailed implementation can be found in the *evaluate()* function under the *ConnectFour* class\n",
    "\n",
    "The heuristic function counts the number of chains with different length (horizontal, vertical or diagonal) for each player. Chains with different length will be assigned different scores (length of 1:1; length of 2:8; length of 3:128, length of 4:99999). The utility of the node will be the difference between the score of the algorithm as a player, and the score of the other player (computer or human).\n",
    "\n",
    "The underlying principle is that a longer chain implies higher chance of winning. Therefore different lengths of chain will have different score weightings. The difference between the scores will signal which player has a higher chance of winning if we continue to simulate the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplistic Monte Carlo Tree Search\n",
    "The limitation of the NegaMax search algorithm (or any variation of the MiniMax search algorithm) is that the number of search steps is usually very large to run through a large amount of simulation in order to obtain the best move. A way to mitigate this problem is using the evaluate heuristic introduced above.\n",
    "\n",
    "Another search algorithm that can mitigate the problem with MiniMax when it encounters high branching factor is Monte Carlo Tree Search. Monte Carlo Tree Search chooses the node with the best result (wins/game simulated) and simulate the rest game based on the action of that best node. Each time it simulates the game, it will reach a conclusion (win, lose or draw). It will then update the win/simulation ratio for all of its parents.\n",
    "\n",
    "The benefits of Monte Carlo Tree Search over NegaMax algorithm in theory are:  \n",
    "1. It can simulate more full games than NegaMax, so to provide a better estimate of the end result given a move\n",
    "2. Every search is a greedy decision: selecting the node with the best outcome currently to explore. It saves a lot of search space compared to NegaMax's depth first search regardless of the outcome of the path\n",
    "\n",
    "### Implementation of Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Queue, random\n",
    "\n",
    "#returns an explored Monte Carlo Tree \n",
    "def mcts(game,state,player,budget):\n",
    "    root = MonteCarloNode(state,None,player,cur_game=game,parent=None)\n",
    "    #FIFO queue to store all the frontier nodes\n",
    "    expQueue = Queue.Queue()\n",
    "    expQueue.put(root)\n",
    "    \n",
    "    for i in xrange(budget):\n",
    "        if expQueue.qsize == 0:\n",
    "            break\n",
    "        \n",
    "        current = expQueue.get()\n",
    "        if current.parent is not None:\n",
    "            current.parent.children[current.action] = current\n",
    "        \n",
    "        #Tree Policy: Expanding the current node to children node\n",
    "        for action in game.actions(current.state):\n",
    "            childAction = action\n",
    "            childPlayer = game.next_player(current.player)\n",
    "            childState = game.update_state(current.state,childAction,childPlayer)\n",
    "            curChild = MonteCarloNode(childState,childAction,childPlayer,cur_game=current.game,parent=current)\n",
    "            expQueue.put(curChild)\n",
    "       \n",
    "       #Default Policy: Run simulation till the end of game to obtain reward\n",
    "        curState = current.state\n",
    "        curPlayer = current.player\n",
    "        game = current.game\n",
    "        while not game.terminal(curState):\n",
    "            curAction = random.choice(game.actions(curState))\n",
    "            curState = game.update_state(curState,curAction,curPlayer)\n",
    "            curPlayer = game.next_player(curPlayer)\n",
    "        reward = game.outcome(curState,player)\n",
    "        \n",
    "        #Back propagation: Propagate the reward result back to the root node\n",
    "        backupNode = current\n",
    "        while backupNode is not None:\n",
    "            backupNode.value += reward\n",
    "            backupNode.visit += 1\n",
    "            backupNode.weight = backupNode.value/backupNode.visit\n",
    "            backupNode = backupNode.parent\n",
    "\n",
    "    bestChild = max(root.children.values(), key=lambda c: c.weight)\n",
    "    return bestChild.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **four steps** to Monte Carlo Tree Search:\n",
    "#### Selection\n",
    "The algorithm will select the node with the highest win/simiulation value to explore. If there is a tie, it will randomly select the node among the nodes that are tied. This process will go on until we reach a leaf node with no child.\n",
    "#### Expansion\n",
    "Once the path reaches a child-less node, it will expand and create a child node. The position (in this case, the action the child will take) is randomly chosen. The new child node will be added to the Monte Carlo Tree.\n",
    "#### Simulation\n",
    "Once a new, randomly chosen child is created, the algorithm will simulate the rest of the game and return an outcome (win, lose or draw). Because a full game have at most 47 steps, each simulation runs a full game. But for games with more average steps (e.g. chess), the outcome will be a estimation similar to **evaluate** in NegaMax.\n",
    "#### Back propagation\n",
    "Once there is an outcome to the simulated game, the algorithm will update the win/simulated value of all the nodes on the path. This will ensure that, when the algorithm is evaluating on the best move to make at one moment, it accounts for all the subsequent simulations given that move.  \n",
    "\n",
    "----------------------------\n",
    "\n",
    "The algorithm is configured to stop after any desired number of simulations (budget). Once the algorithm stops, it will look at the children nodes of the root, and determine the best node based on the win/simulated value, and return the action of the best child node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCT Variant of Monte Carlo Tree Search\n",
    "The problem with simple Monte Carlo Tree Search is that it is at a tug of war with two different goals: exploitation and exploration. \n",
    "\n",
    "On one hand, it wants to exploit the child node of the root with the best win/simulation value, going deeper into the branches of that node. On the other hand, it needs to explore other actions that may give a higher win/simulation value than the ones explored. The simple implementation exploits the child node with the best win/simulation, until there is enough loses to render it suboptimal. However, it may miss the exploration of other nodes that may later on yield better win/simulation ratio\n",
    "\n",
    "The UCT variant of Monte Carlo Tree Search that tries to balance the exploitation and exploration goals of the search algorithm by assigning a search weight to each node based on both the win/simulation ratio and the number of times the node has been visited.\n",
    "\n",
    "### UCT Search Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _search_weight(self, c):\n",
    "    return self._weight() + c * sqrt(2 * log(self.parent.visit) / self.visit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A child node *j* is selected to maximize the UCT search weight, given by this formula: \n",
    "\n",
    "[math formula for UCT]\n",
    "\n",
    "If the child node is not visited, the search weight would be infinity, so that previously unvisited children are searched first. The constant *Cp* determines whether the Monte Carlo Tree Search bias towards exploitation (if *Cp* is small) or exploration (if *Cp* is large). With a reward of (1,-1), the optimal *Cp* is 1/sqrt(2), given by Kocsis and Szepesvari.\n",
    "\n",
    "### Implementation Monte Carlo Tree Search with UCT\n",
    "The UCT variant of Monte Carlo Tree Search is identical as the simplistic Monte Carlo Tree Search, except the best child is selected based on the highest UCT search weight, not the win/simulation ratio.\n",
    "\n",
    "The following implementation replaced the in-function selection, expansion, simulation and back propagation with MonteCarloNode class functions. Other than that, it is identical to the previous Monte Carlo Tree Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mcts_uct(game,state,player,budget,c=(1/sqrt(2))):\n",
    "    root = MonteCarloNode(state,None,player,cur_game=game,parent=None)\n",
    "    computerWin = 0.0\n",
    "    originalBudget = budget\n",
    "    \n",
    "    search_step = max_search_depth = 0\n",
    "    \n",
    "    while budget:\n",
    "        child = root\n",
    "        child = child._tree_policy()\n",
    "        search_step += 1\n",
    "        reward = child._default_policy(player)\n",
    "        search_depth = child._backup(reward,budget)\n",
    "        if search_depth > max_search_depth:\n",
    "            max_search_depth = search_depth\n",
    "        budget-=1\n",
    "     \n",
    "    bestChild = root._best_child(c)\n",
    "    return bestChild.action, search_step, max_search_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negamax vs Monte Carlo Tree Search\n",
    "To evaluate the practical performance of the two major search algorithms. I built a simulation and allow a NegaMax bot play against a UCT-Monte Carlo Tree Search bot. The simulation plays 50 games.\n",
    "\n",
    "The metrics measured after the simulation for both bots are: \n",
    "* Search depth (per move)\n",
    "* branching factor (per move)\n",
    "* search time (per move)\n",
    "* win percentage (over 100 games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTS-UCT won the 1-th game.\n",
      "MCTS-UCT won the 2-th game.\n",
      "MCTS-UCT won the 3-th game.\n",
      "MCTS-UCT won the 4-th game.\n",
      "MCTS-UCT won the 5-th game.\n",
      "MCTS-UCT won the 6-th game.\n",
      "NegaMax won the 7-th game.\n",
      "MCTS-UCT won the 8-th game.\n",
      "MCTS-UCT won the 9-th game.\n",
      "MCTS-UCT won the 10-th game.\n",
      "MCTS-UCT won the 11-th game.\n",
      "MCTS-UCT won the 12-th game.\n",
      "MCTS-UCT won the 13-th game.\n",
      "MCTS-UCT won the 14-th game.\n",
      "MCTS-UCT won the 15-th game.\n",
      "MCTS-UCT won the 16-th game.\n",
      "MCTS-UCT won the 17-th game.\n",
      "NegaMax won the 18-th game.\n",
      "MCTS-UCT won the 19-th game.\n",
      "MCTS-UCT won the 20-th game.\n",
      "MCTS-UCT won the 21-th game.\n",
      "MCTS-UCT won the 22-th game.\n",
      "MCTS-UCT won the 23-th game.\n",
      "MCTS-UCT won the 24-th game.\n",
      "MCTS-UCT won the 25-th game.\n",
      "MCTS-UCT won the 26-th game.\n",
      "NegaMax won the 27-th game.\n",
      "MCTS-UCT won the 28-th game.\n",
      "MCTS-UCT won the 29-th game.\n",
      "NegaMax won the 30-th game.\n",
      "MCTS-UCT won the 31-th game.\n",
      "MCTS-UCT won the 32-th game.\n",
      "MCTS-UCT won the 33-th game.\n",
      "MCTS-UCT won the 34-th game.\n",
      "MCTS-UCT won the 35-th game.\n",
      "MCTS-UCT won the 36-th game.\n",
      "MCTS-UCT won the 37-th game.\n",
      "MCTS-UCT won the 38-th game.\n",
      "MCTS-UCT won the 39-th game.\n",
      "MCTS-UCT won the 40-th game.\n",
      "MCTS-UCT won the 41-th game.\n",
      "MCTS-UCT won the 42-th game.\n",
      "NegaMax won the 43-th game.\n",
      "MCTS-UCT won the 44-th game.\n",
      "NegaMax won the 45-th game.\n",
      "MCTS-UCT won the 46-th game.\n",
      "MCTS-UCT won the 47-th game.\n",
      "MCTS-UCT won the 48-th game.\n",
      "MCTS-UCT won the 49-th game.\n",
      "NegaMax won the 50-th game.\n",
      "\n",
      "===============================================================\n",
      "Here are the results for the simulation\n",
      "                                   NegaMax   vs   MCTS-UCT\n",
      "Win Ratio:                          0.14            0.86\n",
      "Runtime (sec/move):                21.118           0.377\n",
      "Average Search Step (per move):    2588.786        200.000\n",
      "Average Search Depth (per move):   382.292         6.109\n",
      "\n",
      "===============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from numpy import mean\n",
    "\n",
    "\n",
    "noOfSim = 50\n",
    "originalNoOfSim = noOfSim\n",
    "gameCount = 1\n",
    "\n",
    "players = ['','NegaMax','MCTS-UCT']\n",
    "mctsWin = 0.0\n",
    "negaWin = 0.0\n",
    "avgNegaSearchStep = []\n",
    "avgNegaSearchDepth = []\n",
    "avgNegaRunTime = []\n",
    "avgMCTSSearchStep = []\n",
    "avgMCTSSearchDepth = []\n",
    "avgMCTSRunTime = []\n",
    "\n",
    "game = ConnectFour()\n",
    "\n",
    "while noOfSim > 0:\n",
    "    currentState = [[],[],[],[],[],[],[]]\n",
    "    game.state = currentState\n",
    "    player = noOfSim%2+1\n",
    "    currentPlayer = player\n",
    "    \n",
    "    allNegaSearchStep = []\n",
    "    allNegaSearchDepth = []\n",
    "    allNegaRunTime = []\n",
    "    allMctsSearchStep = []\n",
    "    allMctsSearchDepth = []\n",
    "    allMctsRunTime = []\n",
    "    \n",
    "    while not game.terminal(currentState):\n",
    "        if currentPlayer == 1:\n",
    "            start_time = time.time()\n",
    "            negaRunTime = 0.0\n",
    "            negaNode, negaScore, negaSearchStep, negaSearchDepth = negamax(game,currentState,currentPlayer,max_depth=4)\n",
    "            negaRunTime = time.time()-start_time\n",
    "            negaAction = negaNode.action\n",
    "            \n",
    "            if game.legal(currentState,negaAction):\n",
    "                currentState = game.update_state(currentState,negaAction,currentPlayer)\n",
    "                currentPlayer = game.next_player(currentPlayer)\n",
    "            \n",
    "            allNegaSearchStep.append(negaSearchStep)\n",
    "            allNegaSearchDepth.append(negaSearchDepth)\n",
    "            allNegaRunTime.append(negaRunTime)\n",
    "        \n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            mctsRunTime = 0.0\n",
    "            mctsAction, mctsSearchStep, mctsSearchDepth = mcts_uct(game,currentState,currentPlayer,200)  #using the UCT variant of MCTS algorithm\n",
    "            mctsRunTime = time.time()-start_time\n",
    "            \n",
    "            if game.legal(currentState,mctsAction):\n",
    "                currentState = game.update_state(currentState,mctsAction,currentPlayer)\n",
    "                currentPlayer = game.next_player(currentPlayer)\n",
    "            \n",
    "            allMctsSearchStep.append(mctsSearchStep)\n",
    "            allMctsSearchDepth.append(mctsSearchDepth)\n",
    "            allMctsRunTime.append(mctsRunTime)\n",
    "    \n",
    "    avgNegaSearchStep.append(mean(allNegaSearchStep))\n",
    "    avgNegaSearchDepth.append(mean(allNegaSearchDepth))\n",
    "    avgNegaRunTime.append(mean(allNegaRunTime))\n",
    "    \n",
    "    avgMCTSSearchStep.append(mean(allMctsSearchStep))\n",
    "    avgMCTSSearchDepth.append(mean(allMctsSearchDepth))\n",
    "    avgMCTSRunTime.append(mean(allMctsRunTime))\n",
    "    \n",
    "    outcome = game.outcome(currentState,player)\n",
    "    if outcome > 0:\n",
    "        print \"{} won the {}-th game.\".format(players[player],gameCount)\n",
    "        if player == 1: negaWin += 1\n",
    "        else: mctsWin += 1\n",
    "    elif outcome < 0:\n",
    "        if player == 1:\n",
    "            print \"{} won the {}-th game.\".format(players[player+1],gameCount)\n",
    "            mctsWin += 1\n",
    "        else:\n",
    "            print \"{} won the {}-th game.\".format(players[player-1],gameCount)\n",
    "            negaWin += 1\n",
    "    else:\n",
    "        print \"{}-th game is draw.\".format(gameCount)\n",
    "    \n",
    "    gameCount += 1\n",
    "    noOfSim -= 1\n",
    "    \n",
    "#Results\n",
    "negaWinRatio = negaWin/originalNoOfSim\n",
    "mctsWinRatio = mctsWin/originalNoOfSim\n",
    "print \"\\n===============================================================\"\n",
    "print \"Here are the results for the simulation\"\n",
    "print \"                                   NegaMax   vs   MCTS-UCT\"\n",
    "print \"Win Ratio:                          %.2f            %.2f\" %(negaWinRatio,mctsWinRatio)\n",
    "print \"Runtime (sec/move):                %.3f           %.3f\" %(mean(avgNegaRunTime),mean(avgMCTSRunTime))\n",
    "print \"Average Search Step (per move):    %.3f        %.3f\" %(mean(avgNegaSearchStep),mean(avgMCTSSearchStep))\n",
    "print \"Average Search Depth (per move):   %.3f         %.3f\" %(mean(avgNegaSearchDepth),mean(avgMCTSSearchDepth))\n",
    "print \"\\n===============================================================\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the simulation\n",
    "Based on the wins, Monte Carlo Tree Search performs much better than NegaMax. It also has a smaller runtime and smaller tree to search for every move.\n",
    "\n",
    "The reason behind the small search tree is that MCTS is a breadth-first search algorithm, while NegMax is a depth-first search algorithm. MCTS starts from the roots, chooses a child node, runs a full game simulation, and then usually chooses another chld node on the same level (because of the UCT search weights that balances on exploration). NegaMax chooses follows a pathh of the best child down the tree, until it either hits the search depth or the game ends. Therefore the search depth of NegaMax is significant higher than MCTS.\n",
    "\n",
    "A potential explanation on the better performance of MCTS lies on better estimate of the utility of the move and seeking for global optima. As mentioned above, MCTS runs more full game simulation, which provides better estimate of the utility of the move. However, becasue the moves of the simulation are random. The UCT exploration balancing make sure that we explore more variations of moves to attempt to reach a global optima in terms of win/simulation. These two factors made MCTS a much more optimized algorithm in terms of search time and outcome.\n",
    "\n",
    "An interesting phenomenon observed is that, as the simulation progress, the number of wins by NegaMax increased slightly (1 for the first 10 games, 1 in the second 10 games, 2 in the third 10 games, 0 in the forth 10 game, 3 in the fifth 10 game). This phenomenon could be random, or could be that the algorithm remembers different states from previous games and skip through some steps in a full game tree, hence being able to simulate more complete games as it plays more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Gameplay\n",
    "Running the code below allows you to play against either one of the AIFour Connect Four AI algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two variation of AI algorithm you can play with:\n",
      "Negamax (negamax): A simpler variation of Minimax algorithm\n",
      "UCT Variant of MCTS (monteuct): MCTS with the UCT search-weighting function\n"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "currentState = game.state\n",
    "player = 1\n",
    "\n",
    "print \"There are two variation of AI algorithm you can play with:\"\n",
    "print \"Negamax (negamax): A simpler variation of Minimax algorithm\"\n",
    "print \"UCT Variant of MCTS (monteuct): MCTS with the UCT search-weighting function\"\n",
    "selection = str(raw_input(\"\\nWhich AI algorithm do you want to play against (negamax,monteuct): \"))\n",
    "\n",
    "print \"\\nYou are playing with an AI agent that uses {}\".format(selection)\n",
    "print \"\\n====================================================\\n\"\n",
    "    \n",
    "currentPlayer = player\n",
    "while not game.terminal(currentState):\n",
    "    if currentPlayer == 1:\n",
    "        humanAction = int(raw_input(\"Choose a column: \"))\n",
    "        if game.legal(currentState,humanAction):\n",
    "            currentState = game.update_state(currentState,humanAction,currentPlayer)\n",
    "            currentPlayer = game.next_player(currentPlayer)\n",
    "            print \"You played column {}\".format(humanAction)\n",
    "    else:\n",
    "        if selection == \"monteuct\":\n",
    "            computerAction,searchStep,searchDepth = mcts_uct(game,currentState,currentPlayer,2000)  #using the UCT variant of MCTS algorithm\n",
    "        elif selection == \"negamax\":\n",
    "            computerNode, computerScore,searchStep,searchDepth = negamax(game,currentState,currentPlayer,max_depth=4)\n",
    "            computerAction = computerNode.action\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Choice of the AI algorithm\")\n",
    "        \n",
    "        if game.legal(currentState,computerAction):\n",
    "            currentState = game.update_state(currentState,computerAction,currentPlayer)\n",
    "            currentPlayer = game.next_player(currentPlayer)\n",
    "            print \"The computer played column {}\".format(computerAction)\n",
    "    \n",
    "    currentBoard = game.pretty_state(currentState)\n",
    "    print currentBoard\n",
    "    \n",
    "outcome = game.outcome(currentState,player)\n",
    "if outcome > 0:\n",
    "    print \"You won!\"\n",
    "elif outcome < 0:\n",
    "    print \"You lose!\"\n",
    "else:\n",
    "    print \"Draw!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "GUI for connect four: https://github.com/uroshekic/connect-four/blob/master/ConnectFour.py  \n",
    "\n",
    "Nega-max Algorithm: http://blog.gamesolver.org/solving-connect-four/03-minmax/  \n",
    "Explanation of Nega-max Algorithm: http://www.hamedahmadi.com/gametree/#negamax\n",
    "\n",
    "Monte Carlo Tree Search Algorithm: http://mcts.ai/pubs/mcts-survey-master.pdf  \n",
    "Sample Implementation of Monte Carlo Tree Search: https://jeffbradberry.com/posts/2015/09/intro-to-monte-carlo-tree-search/\n",
    "More Monte Carlo Implementation: https://mattschmoyer.com/connect-four-ai/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
